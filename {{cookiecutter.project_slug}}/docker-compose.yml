# https://geshan.com.np/blog/2025/02/ollama-docker-compose/
services:
  proxy:
    image: traefik:v3.4
    container_name: proxy
    ports:
      - "80:80"
      #- "443:443"
    environment:
    #   - CF_API_EMAIL=$CLOUDFLARE_EMAIL
    #   - CF_DNS_API_TOKEN=$CLOUDFLARE_API_KEY
      - TZ=${TZ}
      - ROOT_DOMAIN=${ROOT_DOMAIN}
    profiles:
      - proxy
      - llminabox
      - auth
      - chat-ui
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock:ro"
      - ./services/proxy/rules:/rules
      - proxy_logs:/logs
      #- proxy_certs:/letsencrypt
    networks:
       - proxy_net
    restart: unless-stopped
    command:
      # TODO disable
      # - --log.level=DEBUG
      - "--providers.docker=true"
      - --providers.docker.endpoint=unix:///var/run/docker.sock # Use Docker Socket Proxy instead for improved security
      - "--providers.docker.exposedbydefault=false"
      - "--providers.docker.network=proxy_net"
      - "--entrypoints.web.address=:80"
      #- --accesslog.filepath=/logs/access.log
      - "--entrypoints.web.asdefault=true"
    #   - "--entrypoints.websecure.address=:443"
    #   - --entrypoints.websecure.http.tls.options=tls-opts@file
    #   # Allow these IPs to set the X-Forwarded-* headers - Cloudflare IPs: https://www.cloudflare.com/ips/
    #   - --entrypoints.websecure.forwardedHeaders.trustedIPs=$CLOUDFLARE_IPS,$LOCAL_IPS

      - --providers.file.directory=/rules
      - --providers.file.watch=true # Only works on top level files in the rules folder
      #- "--providers.file.filename=/etc/traefik/dynamic_conf.toml"
      #- "--providers.file.filename=/rules/dynamic_conf.toml"
    #   - --certificatesResolvers.dns-cloudflare.acme.email=$CF_API_EMAIL
    #   - --certificatesResolvers.dns-cloudflare.acme.storage=/letsencrypt/acme.json
    #   - --certificatesResolvers.dns-cloudflare.acme.dnsChallenge.provider=cloudflare
    #   - --certificatesResolvers.dns-cloudflare.acme.dnsChallenge.resolvers=1.1.1.1:53,1.0.0.1:53
    #   - --certificatesResolvers.dns-cloudflare.acme.dnsChallenge.delayBeforeCheck=90 # To delay DNS check and reduce LE hitrate
    #   - --entryPoints.web.http.redirections.entrypoint.to=websecure
    #   - --entryPoints.web.http.redirections.entrypoint.scheme=https
  # auth:
  #   image: quay.io/keycloak/keycloak:26.2
  #   depends_on:
  #     authdb:
  #       condition: service_healthy
  #   container_name: keycloak
  #   restart: unless-stopped
  #   # Just-in-time build + runtime options
  #   command: >
  #     start
  #     --db=postgres
  #     --hostname auth.${ROOT_DOMAIN}
  #     --http-enabled=true
  #     --proxy-headers=xforwarded
  #     --health-enabled=true
  #     --metrics-enabled=true

  #   environment:
  #     KC_DB_URL: jdbc:postgresql://authdb:5432/${AUTH_DB}
  #     KC_DB_USERNAME: ${AUTH_DB_USER}
  #     KC_DB_PASSWORD: ${AUTH_DB_PASSWORD}
  #     KC_LOG_LEVEL: INFO
  #     KC_BOOTSTRAP_ADMIN_USERNAME: ${KC_BOOTSTRAP_ADMIN_USERNAME}
  #     KC_BOOTSTRAP_ADMIN_PASSWORD: ${KC_BOOTSTRAP_ADMIN_PASSWORD}
  #   #ports:
  #   #  - "8080:8080"   # HTTP left open for the reverse-proxy on the same LAN
  #   #  - "9000:9000"   # management port: /health and /metrics
  #   profiles:
  #     - auth
  #   networks:
  #     - proxy_net
  #     - auth_net
    
  modelserverollama-cpu:
    image: ollama/ollama:0.9.6
    volumes:
      - ollama:/root/.ollama
    container_name: ollama
    restart: unless-stopped
    profiles:
      - ollama-cpu
      - llx
    networks:
      model_router_net:
        aliases:
          - ollama
  modelserverollama-gpu:
    image: ollama/ollama:0.9.6
    #ports:
    #  - 11434:11434
    volumes:
      - ollama:/root/.ollama
    container_name: ollama
    restart: unless-stopped
    profiles:
      - ollama-gpu
    networks:
      model_router_net:
        aliases:
          - ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # modelservervllm:
  #   build:
  #   # docker build -f docker/Dockerfile.cpu -t vllm-cpu-env --shm-size=4g .
  #     context: ./services/model-server/vllm
  #     # TODO select the right architecture/accelerator support here
  #     # TODO document & expose as easy to use docker compose profile flags
  #     # TODO add gpu claims here in case of GPU support
  #     dockerfile: docker/Dockerfile.arm
  #     target: build
  #     shm_size: "4gb"
  #     args:
  #       # This version must match the checked out vllm version for the submodule
  #       SETUPTOOLS_SCM_PRETEND_VERSION: "0.9.1"
  #   image: llminabox/modelserver:v0.9.1-cpu-arm
  #   container_name: modelserver
  #   hostname: modelserver
  #   environment:
  #     HF_HOME: /models/.cache
  #   # cpuset: "0-3"
  #   command:
  #     - "--model"
  #     - "NousResearch/Meta-Llama-3-8B-Instruct"
  #     - "--port"
  #     - "8000"
  #   # restart: unless-stopped
  #   ports:
  #     - "8000:8000"
  #   profiles:
  #     - model-server-cpu-arm
  #   volumes:
  #     - ./z_tmp/models:/models

  # authdb:
  #   image: postgres:17.5-alpine3.22
  #   container_name: authdb
  #   restart: unless-stopped
  #   environment:
  #     POSTGRES_DB: ${AUTH_DB}
  #     POSTGRES_USER: ${AUTH_DB_USER}
  #     POSTGRES_PASSWORD: ${AUTH_DB_PASSWORD}
  #   healthcheck:
  #     test: [ "CMD-SHELL", "pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}" ]
  #     interval: 5s
  #     timeout: 10s
  #     retries: 10
  #   profiles:
  #     - auth
  #   volumes:
  #     - llm_model_auth_db:/var/lib/postgresql/data
  #   networks:
  #      - auth_net
  routerdb:
    image: postgres:17.5-alpine3.22
    container_name: routerdb
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${LLM_ROUTER_DB}
      POSTGRES_USER: ${LLM_ROUTER_DB_USER}
      POSTGRES_PASSWORD: ${LLM_ROUTER_DB_PASSWORD}
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}" ]
      interval: 5s
      timeout: 10s
      retries: 10
    profiles:
      - model-router
      - llminabox
      - llx
    volumes:
      - llm_model_router_db:/var/lib/postgresql/data
    networks:
       - model_router_net

  llm_router:
    image: litellm/litellm:v1.72.0-stable
    platform: linux/amd64
    container_name: llmrouter
    restart: unless-stopped
    environment:
      - LITELLM_MASTER_KEY=sk-${LITELLM_MASTER_KEY}
      - LITELLM_SALT_KEY=${LITELLM_SALT_KEY}
      - STORE_MODEL_IN_DB="True"
      - DATABASE_URL=postgres://${LLM_ROUTER_DB_USER}:${LLM_ROUTER_DB_PASSWORD}@routerdb:5432/${LLM_ROUTER_DB}
      - OPENAI_API_KEY=${ROUTER_OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ROUTER_ANTHROPIC_API_KEY}
      - LITELLM_MODE="PRODUCTION"
      - UI_USERNAME=${LITELLM_UI_USERNAME}
      - UI_PASSWORD=${LITELLM_UI_PASSWORD}
    profiles:
      - model-router
      - llminabox
      - llx
    networks:
      - model_router_net
      - llm_net
      - proxy_net
    expose:
      - "4000"
    depends_on:
      routerdb:
        condition: service_healthy
      #- ollama
    healthcheck:
      test: [ "CMD-SHELL", "wget --no-verbose --tries=1 http://localhost:4000/health/liveliness || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    #ports:
    #  - "4000:4000"
    # :4000 is api; /ui -> UI
    volumes:
      - ./services/llm-router/litellm_config.yml:/app/config.yaml
      - ./services/llm-router/google_vertexai.json:/secrets/google_vertexai.json
    #  --detailed_debug
    command: [ "--config", "/app/config.yaml", "--port", "4000", "--num_workers", "2" ]
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.litellm.rule=Host(`llm.${ROOT_DOMAIN}`)"
      - "traefik.http.services.litellm.loadbalancer.server.port=4000"
    #   - "traefik.http.routers.litellm.tls.certresolver=dns-cloudflare"
      - "traefik.http.routers.litellm.middlewares=chain-no-auth@file"

  chatuidb:
    image: postgres:17.5-alpine3.22
    container_name: chatuidb
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${CHAT_UI_DB}
      POSTGRES_USER: ${CHAT_UI_DB_USER}
      POSTGRES_PASSWORD: ${CHAT_UI_DB_PASSWORD}
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}" ]
      interval: 5s
      timeout: 10s
      retries: 10
    profiles:
      - chat-ui
      - llminabox
    volumes:
      - llm_chat_ui_db:/var/lib/postgresql/data
    networks:
       - chatui_net

  docling-cpu:
    image: quay.io/docling-project/docling-serve-cpu:v1.0.0
    container_name: docling
    #ports:
    #  - 5001:5001
    profiles:
      - docling-cpu
    networks:
      - chatui_net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/health"]
      interval: 15s
      retries: 5

  docling-gpu:
    image: quay.io/docling-project/docling-serve-cu124:v1.0.0
    container_name: docling
    #ports:
    #  - 5001:5001
    profiles:
      - docling-gpu
    networks:
      - chatui_net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/health"]
      interval: 15s
      retries: 5
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  vectordb-qdrant-cpu:
    image: qdrant/qdrant:v1.14.1
    container_name: qdrant
    hostname: qdrant
    restart: unless-stopped
    environment:
      QDRANT__SERVICE__API_KEY: ${QDRANT__SERVICE__API_KEY}
    profiles:
      - vectordb-cpu
    networks:
      - chatui_net
    # configs:
    #   - source: qdrant_config
    #     target: /qdrant/config/production.yaml
    volumes:
      - qdrant:/qdrant/storage
  # vectordb-qdrant-cpu-healthcheck:
  #   restart: always 
  #   image: curlimages/curl:latest
  #   entrypoint: ["/bin/sh", "-c", "--", "while true; do sleep 30; done;"]
  #   profiles:
  #     - vectordb-cpu
  #   depends_on:
  #     - qdrant
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://qdrant:6333/readyz"]
  #     interval: 10s
  #     timeout: 2s
  #     retries: 5
  llm_chat_ui:
  # TODO: create GPU variation as well for faster whisper and embeddings
  # TODO add a vector database elastic? qdrant?
    image: ghcr.io/open-webui/open-webui:v0.6.16
    container_name: chatui
    volumes:
      - open-webui:/app/backend/data
    depends_on:
      chatuidb:
        condition: service_healthy
      llm_router:
        condition: service_healthy
    environment:
      WEBUI_SECRET_KEY: ${CHAT_UI_SECRET_KEY}
      CORS_ALLOW_ORIGIN: http://chat.${ROOT_DOMAIN}
      CONTENT_EXTRACTION_ENGINE: docling
      DOCLING_SERVER_URL: http://docling:5001
      DATABASE_URL: postgresql://${CHAT_UI_DB_USER}:${CHAT_UI_DB_PASSWORD}@chatuidb:5432/${CHAT_UI_DB}
      ENABLE_IMAGE_GENERATION: "True"
      ENABLE_CHANNELS: "True"
      IMAGE_GENERATION_ENGINE: "openai"
      IMAGE_SIZE: "1024x1024"
      IMAGE_GENERATION_MODEL: "dall-e-3"
      ENABLE_OLLAMA_API: "False"
      ENABLE_OPENAI_API: "True"
      #OPENAI_API_BASE_URLS: "https://api.openai.com/v1;http://llmrouter:4000/v1"
      #OPENAI_API_KEYS: "${ROUTER_OPENAI_API_KEY};${LLM_ROUTER_KEY_FOR_CHAT_UI}"
      IMAGES_OPENAI_API_KEY: "${ROUTER_OPENAI_API_KEY}"
      VECTOR_DB: qdrant
      QDRANT_API_KEY: ${QDRANT__SERVICE__API_KEY}
      QDRANT_URI: http://qdrant:6333
      ENABLE_QDRANT_MULTITENANCY_MODE: "True"
      # ENABLE_OPENAI_API, OPENAI_API_KEY, OPENAI_API_KEYS
      # VECTOR_DB, ELASTICSEARCH_URL
  #     - 'OLLAMA_BASE_URL=http://ollama:11434'
    restart: unless-stopped
    profiles:
       - chat-ui
       - llminabox
    networks:
      - proxy_net
      - llm_net
      - chatui_net
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.chat.rule=Host(`chat.${ROOT_DOMAIN}`)"
      - "traefik.http.routers.chat.service=chat"
      - "traefik.http.services.chat.loadbalancer.server.port=8080"
      - "traefik.http.routers.chat.middlewares=chain-no-auth@file"

networks:
  proxy_net:
    name: proxy_net
    # external: true
  model_router_net:
  auth_net:
  llm_net:
  chatui_net:
  
volumes:
  ollama: {}
  llm_model_router_db: {}
  #llm_model_auth_db: {}
  llm_chat_ui_db: {}
  proxy_logs: {}
  proxy_certs: {}
  open-webui: {}
  qdrant: {}
